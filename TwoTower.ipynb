{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv folder\n",
    "extract_path = '../visit_1/visit_1'\n",
    "csv_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]\n",
    "\n",
    "csv_files = csv_files[:int(len(csv_files)*0.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel file\n",
    "xlxs_path = '../shhs1_ahi_pruebas.xlsx'\n",
    "xlxs_data = pd.read_excel(xlxs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_csv(csv_file, extract_path):\n",
    "    match = re.search(r'(\\d+)_extraction', csv_file)\n",
    "    if match:\n",
    "        current_id = match.group(1)\n",
    "        current_data = pd.read_csv(os.path.join(extract_path, csv_file))\n",
    "        current_data['ID'] = current_id\n",
    "        return current_data\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use parallel processors\n",
    "num_processes = 6 #change any number of processes you want to use\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "processed_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(process_csv)(csv_file, extract_path)\n",
    "    for csv_file in tqdm(csv_files)\n",
    ")\n",
    "\n",
    "all_data = pd.concat([df for df in processed_data if not df.empty], ignore_index=True) # type: ignore\n",
    "\n",
    "channels = ['H.R.', 'SaO2', 'ABDO RES', 'THOR RES', 'AIRFLOW','ID']\n",
    "all_data = all_data[channels]\n",
    "\n",
    "# Reset index after concatenation\n",
    "all_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data that channel as feature, ahi as label\n",
    "xlxs_data['ID'] = xlxs_data['ID'].astype(int)\n",
    "all_data['ID'] = all_data['ID'].astype(int)\n",
    "\n",
    "merged_data = pd.merge(all_data, xlxs_data, on='ID', how='left')\n",
    "\n",
    "\n",
    "merged_data['nsrr_ahi_hp3r_aasm15'] = merged_data.groupby('ID')['nsrr_ahi_hp3r_aasm15'].transform('first')\n",
    "merged_data['nsrr_ahi_hp4u_aasm15'] = merged_data.groupby('ID')['nsrr_ahi_hp4u_aasm15'].transform('first')\n",
    "'''\n",
    "merged_data = merged_data.drop(['nsrr_ahi_hp4u_aasm15 ', 'nsrr_ahi_hp3r_aasm15'], axis=1)\n",
    "\n",
    "'''\n",
    "merged_data = merged_data.fillna(0)\n",
    "\n",
    "merged_data = merged_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process features and labels\n",
    "import numpy as np\n",
    "\n",
    "X = merged_data[['H.R.', 'SaO2', 'ABDO RES', 'THOR RES', 'AIRFLOW']].values\n",
    "Y = merged_data['nsrr_ahi_hp3r_aasm15'].values\n",
    "\n",
    "def categorize_ahi(ahi):\n",
    "  if ahi < 5:\n",
    "    return 0 # no sleep apnea\n",
    "  elif ahi < 15:\n",
    "    return 1 # mild\n",
    "  elif ahi < 30:\n",
    "    return 2 # moderate\n",
    "  else:\n",
    "    return 3 # severe\n",
    "\n",
    "processed_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(categorize_ahi)(ahi) for ahi in tqdm(Y)\n",
    ")\n",
    "\n",
    "Y = np.array(processed_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/content/drive/MyDrive/transformer_data\"\n",
    "os.makedirs(directory, exist_ok = True)\n",
    "x_dir = os.path.join(directory,\"x.npy\")\n",
    "y_dir = os.path.join(directory,\"y.npy\")\n",
    "np.save(x_dir, X)\n",
    "np.save(y_dir, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/content/drive/MyDrive/transformer_data\"\n",
    "x_dir = os.path.join(directory,\"x.npy\")\n",
    "y_dir = os.path.join(directory,\"y.npy\")\n",
    "X = np.load(x_dir)\n",
    "Y = np.load(y_dir)\n",
    "Y_bin = label_binarize(Y, classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mat data\n",
    "mat_path = '../Final_Aout/sleep_heart_fractional_visit_1_12_partition_original.mat'\n",
    "mat_data = scipy.io.loadmat(mat_path)\n",
    "new_input_data = mat_data['data']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "new_input_data = scaler.fit_transform(new_input_data)\n",
    "new_input_data = new_input_data.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y_bin, test_size=0.2, random_state=42)\n",
    "X_train_new = new_input_data[:len(X_train)]\n",
    "X_val_new = new_input_data[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the Dataset\n",
    "class VisitDataset(Dataset):\n",
    "    def __init__(self, features1, features2, labels):\n",
    "        self.features1 = features1\n",
    "        self.features2 = features2\n",
    "        self.labels = labels.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features1[idx], self.features2[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisitDataset(X_train, X_train_new, Y_train)\n",
    "test_dataset = VisitDataset(X_val, X_val_new, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20480, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20480, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two-tower transformer\n",
    "class TwoTowerTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.embedding2 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.transformer1 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.transformer2 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.embedding1(x1)\n",
    "        x2 = self.embedding2(x2)\n",
    "        x1 = x1.permute(1, 0, 2)\n",
    "        x2 = x2.permute(1, 0, 2)\n",
    "        x1 = self.transformer1(x1)\n",
    "        x2 = self.transformer2(x2)\n",
    "        x1 = x1.permute(1, 0, 2).reshape(x1.size(1), -1)\n",
    "        x2 = x2.permute(1, 0, 2).reshape(x2.size(1), -1)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 512\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "device_id = 0\n",
    "device = torch.device('cuda:{}'.format(device_id)) if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = TwoTowerTransformer(input_dim, hidden_dim, num_heads, num_layers, num_classes).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    for inputs1, inputs2, labels in tqdm(train_loader):\n",
    "        inputs1 = inputs1.unsqueeze(1).to(torch.float32)\n",
    "        inputs2 = inputs2.unsqueeze(1).to(torch.float32)\n",
    "        inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, AUC: {auc_score:.4f}')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'temp_epoch{epoch}.pth')\n",
    "        print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "for inputs1, inputs2, labels in test_loader:\n",
    "    inputs1 = inputs1.unsqueeze(1).to(torch.float32)\n",
    "    inputs2 = inputs2.unsqueeze(1).to(torch.float32)\n",
    "    inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "    outputs = model(inputs1, inputs2)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    all_outputs.append(outputs.detach().cpu().numpy())\n",
    "    all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "all_outputs = np.concatenate(all_outputs)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "print(f\"Validation Loss: {total_loss / len(test_loader):.4f}, Validation AUC: {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
