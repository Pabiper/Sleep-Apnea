{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"transformer_data\"\n",
    "x_dir = os.path.join(directory,\"x.npy\")\n",
    "y_dir = os.path.join(directory,\"y.npy\")\n",
    "X = np.load(x_dir)\n",
    "Y = np.load(y_dir)\n",
    "print(X)\n",
    "print(Y.shape)\n",
    "Y_bin = label_binarize(Y, classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y_bin, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the Dataset\n",
    "class VisitDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisitDataset(X_train, Y_train)\n",
    "test_dataset = VisitDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20480, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20480, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        #self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.embedding = nn.Linear(input_dim,hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim),\n",
    "            num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2).reshape(x.size(1), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            #self.save_checkpoint(model)\n",
    "        elif val_loss > self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            #self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Saving model...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit\n",
    "\n",
    "def calculate_auc(y_true, y_scores):\n",
    "    y_scores = expit(y_scores)\n",
    "\n",
    "    auc_total = 0.0\n",
    "    num_labels = y_true.shape[1]\n",
    "    valid_labels = 0\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        if np.sum(y_true[:, i]) == 0 or np.sum(y_true[:, i]) == len(y_true[:, i]):\n",
    "            print(f\"Warning: No positive or negative samples in y_true for label {i}, skipping this label.\")\n",
    "            continue\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_scores[:, i])\n",
    "        if len(fpr) > 1:\n",
    "            auc_total += auc(fpr, tpr)\n",
    "            valid_labels += 1\n",
    "        else:\n",
    "            print(f\"Warning: Not enough data to calculate AUC for label {i}\")\n",
    "    \n",
    "    if valid_labels == 0:\n",
    "        return float('nan')\n",
    "    return auc_total / valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 512\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "device_id = 0\n",
    "device = torch.device('cuda:{}'.format(device_id)) if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = Transformer(input_dim, hidden_dim, num_heads, num_layers, num_classes).to(device)\n",
    "\n",
    "pretrained_path = 'best_model.pth'\n",
    "model.load_state_dict(torch.load(pretrained_path))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and validation\n",
    "num_epochs = 300\n",
    "best_loss = 1.0\n",
    "best_auc = 0.0\n",
    "#log_file = open(\"training_log_new.txt\", \"w\")\n",
    "log_file = open(\"log_transformer.txt\", \"w\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.unsqueeze(1).to(torch.float32)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    #train_auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "    unique_labels = np.unique(all_labels, axis=0)\n",
    "    if len(unique_labels) > 1:\n",
    "        train_auc_score = calculate_auc(all_labels, all_outputs)\n",
    "    else:\n",
    "        train_auc_score = 0.0 \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_train_loss:.4f}, AUC: {train_auc_score:.4f}')\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_all_outputs = []\n",
    "    val_all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in test_loader:\n",
    "            val_inputs = val_inputs.unsqueeze(1).to(torch.float32)\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_total_loss += val_loss.item()\n",
    "            val_all_outputs.append(val_outputs.detach().cpu().numpy())\n",
    "            val_all_labels.append(val_labels.detach().cpu().numpy())\n",
    "\n",
    "    val_all_outputs = np.concatenate(val_all_outputs)\n",
    "    val_all_labels = np.concatenate(val_all_labels)\n",
    "\n",
    "    val_auc_score = calculate_auc(val_all_labels, val_all_outputs)\n",
    "    \n",
    "    avg_val_loss = val_total_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation AUC: {val_auc_score:.4f}\")\n",
    "    \n",
    "    # Logging\n",
    "    log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} Train AUC: {train_auc_score:.4f} Val Loss: {avg_val_loss:.4f} Val AUC: {val_auc_score:.4f}\\n\")\n",
    "    \n",
    "    \n",
    "    if avg_train_loss < best_loss:\n",
    "        best_loss = avg_train_loss\n",
    "        torch.save(model.state_dict(), f'best_model_new.pth')\n",
    "        print(\"Best model saved\")\n",
    "    \n",
    "    \n",
    "    best_auc = max(best_auc, val_auc_score)\n",
    "    log_file.write(f'Best loss: {best_loss}, Best AUC: {best_auc}\\n\\n')\n",
    "    log_file.flush()\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stopping(avg_train_loss, model):\n",
    "        print('Early stopping triggered')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # torch.save(model.state_dict(), f'transformer_epoch{epoch}.pth')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "\n",
    "print(f'Best loss is {best_loss}, best AUC is {best_auc}')\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.unsqueeze(1).to(torch.float32)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    all_outputs.append(outputs.detach().cpu().numpy())\n",
    "    all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "all_outputs = np.concatenate(all_outputs)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "print(f\"Validation Loss: {total_loss / len(test_loader):.4f}, Validation AUC: {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2317310fb273c710267c74ef29c9b393507793e397ff0256299cc1e6ceae5a2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
