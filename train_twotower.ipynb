{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "directory = \"transformer_data\"\n",
    "labels = np.load(os.path.join(directory, \"labels.npy\"))\n",
    "aout_data_array = np.load(os.path.join(directory, \"aout_data_3d.npy\"))\n",
    "visit_data_array = np.load(os.path.join(directory, \"visit_data.npy\"))\n",
    "\n",
    "\n",
    "indices = np.random.permutation(len(labels))\n",
    "visit_data_array = visit_data_array[indices]\n",
    "aout_data_array = aout_data_array[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aout_data_array.shape)\n",
    "print(visit_data_array.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels\n",
    "labels = label_binarize(labels, classes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train dataset\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(np.arange(len(labels)), labels, test_size=0.2, random_state=42)\n",
    "visit_train, visit_val = visit_data_array[X_train], visit_data_array[X_val]\n",
    "aout_train, aout_val = aout_data_array[X_train], aout_data_array[X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisitDataset(Dataset):\n",
    "    def __init__(self, visit_features, aout_features, labels):\n",
    "        self.visit_features = visit_features\n",
    "        self.aout_features = aout_features\n",
    "        self.labels = labels.astype(np.float32)\n",
    "        #self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.visit_features[idx], self.aout_features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisitDataset(visit_train, aout_train, Y_train)\n",
    "val_dataset = VisitDataset(visit_val, aout_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerTransformer(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, hidden_dim1, hidden_dim2, hidden_dim3, num_heads, num_layers, num_classes, seq_len1, seq_len2):\n",
    "        super(TwoTowerTransformer, self).__init__()\n",
    "        self.embedding1 = nn.Linear(input_dim1, hidden_dim1)\n",
    "        self.embedding2 = nn.Linear(input_dim2, hidden_dim2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(seq_len1)\n",
    "        self.bn2 = nn.BatchNorm1d(seq_len2)\n",
    "        self.transformer1 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim1, num_heads, hidden_dim1),\n",
    "            num_layers=num_layers)\n",
    "        self.transformer2 = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim2, num_heads, hidden_dim2),\n",
    "            num_layers=num_layers)\n",
    "        self.final_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim3, num_heads, hidden_dim1 + hidden_dim2),\n",
    "            num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.dense1 = nn.Linear(seq_len1, seq_len1)\n",
    "        self.dense2 = nn.Linear(seq_len2, seq_len2)\n",
    "        self.fc = nn.Linear(hidden_dim3 * (seq_len1+ seq_len2), num_classes)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.embedding1(x1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "\n",
    "        x2 = self.embedding2(x2)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x1 = x1.permute(0, 2, 1)  \n",
    "        x1 = self.dense1(x1) \n",
    "        x1 = x1.permute(2, 0, 1)  \n",
    "\n",
    "        x2 = x2.permute(0, 2, 1)  \n",
    "        x2 = self.dense2(x2)  \n",
    "        x2 = x2.permute(2, 0, 1)  \n",
    "\n",
    "        x1 = self.transformer1(x1)\n",
    "        x2 = self.transformer2(x2)\n",
    "\n",
    "        x1 = x1.permute(1, 0, 2)\n",
    "        x2 = x2.permute(1, 0, 2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.final_transformer(x)\n",
    "        x = x.reshape(x.size(1), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            #self.save_checkpoint(model)\n",
    "        elif val_loss > self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            #self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Saving model...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim1 = 5\n",
    "input_dim2 = 5\n",
    "hidden_dim1 = 512\n",
    "hidden_dim2 = 512\n",
    "hidden_dim3 = 512\n",
    "seq_len1 = 36000\n",
    "seq_len2 = 5\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "device_id = 0\n",
    "device = torch.device('cuda:{}'.format(device_id)) if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model = TwoTowerTransformer(input_dim1, input_dim2, hidden_dim1, hidden_dim2, hidden_dim3, num_heads, num_layers, num_classes, seq_len1, seq_len2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_params = count_trainable_parameters(model)\n",
    "print(f'Trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model, device_ids=[0,1])\n",
    "\n",
    "model = model.to(device)\n",
    "# pretrained_path = '../Sleep_Apnea/temp_epoch10.pth'\n",
    "# model.load_state_dict(torch.load(pretrained_path))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_loss = 1.0\n",
    "best_auc = 0.0\n",
    "log_file = open(\"log_twotower.txt\", \"w\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for inputs1, inputs2, labels in tqdm(train_loader):\n",
    "        inputs1 = inputs1.to(torch.float32)\n",
    "        inputs2 = inputs2.to(torch.float32)\n",
    "        inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs1, inputs2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_outputs.append(outputs.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        '''\n",
    "        _,predictions = outputs.max(1)\n",
    "        correct += predictions.eq(labels).sum().item()\n",
    "        total_sample += labels.size(0)\n",
    "        '''\n",
    "    scheduler.step()\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # AUC and Accuracy\n",
    "    train_auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {total_loss / len(train_loader):.4f}, Train AUC: {train_auc_score:.4f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    best_loss = min(best_loss, total_loss)\n",
    "    best_auc = max(best_auc, train_auc_score)\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_all_outputs = []\n",
    "    val_all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_inputs2, val_labels in val_loader:\n",
    "            val_inputs1 = val_inputs1.to(torch.float32)\n",
    "            val_inputs2 = val_inputs2.to(torch.float32)\n",
    "            val_inputs1, val_inputs2, val_labels = val_inputs1.to(device), val_inputs2.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs1, val_inputs2)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_total_loss += val_loss.item()\n",
    "            val_all_outputs.append(val_outputs.detach().cpu().numpy())\n",
    "            val_all_labels.append(val_labels.detach().cpu().numpy())\n",
    "\n",
    "    val_all_outputs = np.concatenate(val_all_outputs)\n",
    "    val_all_labels = np.concatenate(val_all_labels)\n",
    "\n",
    "    val_auc_score = roc_auc_score(val_all_labels, val_all_outputs)\n",
    "    \n",
    "    avg_val_loss = val_total_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation AUC: {val_auc_score:.4f}\")\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(\"Best model saved\")\n",
    "\n",
    "    log_file.write(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} Train AUC: {train_auc_score:.4f} Val Loss: {avg_val_loss:.4f} Val AUC: {val_auc_score:.4f}\\n\")\n",
    "    log_file.write(f'Best loss: {best_loss}, Best AUC: {best_auc}\\n\\n')\n",
    "    log_file.flush()\n",
    "\n",
    "    if early_stopping(avg_loss, model):\n",
    "        print('Early stopping triggered')\n",
    "        break\n",
    "\n",
    "print(f'Best loss is {best_loss}, best AUC is {best_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "for inputs1, inputs2, labels in val_loader:\n",
    "    inputs1 = inputs1.to(torch.float32)\n",
    "    inputs2 = inputs2.to(torch.float32)\n",
    "    inputs1, inputs2, labels = inputs1.to(device), inputs2.to(device), labels.to(device)\n",
    "    outputs = model(inputs1, inputs2)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    all_outputs.append(outputs.detach().cpu().numpy())\n",
    "    all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "all_outputs = np.concatenate(all_outputs)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "auc_score = roc_auc_score(all_labels, all_outputs, multi_class='ovr')\n",
    "print(f\"Validation Loss: {total_loss / len(val_loader):.4f}, Validation AUC: {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2317310fb273c710267c74ef29c9b393507793e397ff0256299cc1e6ceae5a2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
