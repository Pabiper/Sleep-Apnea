{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv folder\n",
    "\n",
    "\"\"\"\n",
    "import zipfile\n",
    "zip_path = '/content/drive/MyDrive/visit_1.zip'\n",
    "save_path = '/content/visit_1'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "  os.makedirs(save_path)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "  zip_ref.extractall(save_path)\n",
    "\"\"\"\n",
    "extract_path = '../visit_1'\n",
    "csv_files = [f for f in os.listdir(extract_path) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel file\n",
    "xlxs_path = '../shhs1_ahi_pruebas.xlsx'\n",
    "xlxs_data = pd.read_excel(xlxs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_csv(csv_file, extract_path):\n",
    "    match = re.search(r'(\\d+)_extraction', csv_file)\n",
    "    if match:\n",
    "        current_id = match.group(1)\n",
    "        current_data = pd.read_csv(os.path.join(extract_path, csv_file))\n",
    "        current_data['ID'] = current_id\n",
    "        return current_data\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use parallel processors\n",
    "num_processes = 6 #change any number of processes you want to use\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "processed_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(process_csv)(csv_file, extract_path)\n",
    "    for csv_file in tqdm(csv_files)\n",
    ")\n",
    "\n",
    "all_data = pd.concat([df for df in processed_data if not df.empty], ignore_index=True) # type: ignore\n",
    "\n",
    "channels = ['H.R.', 'SaO2', 'ABDO RES', 'THOR RES', 'AIRFLOW','ID']\n",
    "all_data = all_data[channels]\n",
    "\n",
    "# Reset index after concatenation\n",
    "all_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data that channel as feature, ahi as label\n",
    "xlxs_data['ID'] = xlxs_data['ID'].astype(int)\n",
    "all_data['ID'] = all_data['ID'].astype(int)\n",
    "\n",
    "merged_data = pd.merge(all_data, xlxs_data, on='ID', how='left')\n",
    "\n",
    "\n",
    "merged_data['nsrr_ahi_hp3r_aasm15'] = merged_data.groupby('ID')['nsrr_ahi_hp3r_aasm15'].transform('first')\n",
    "merged_data['nsrr_ahi_hp4u_aasm15'] = merged_data.groupby('ID')['nsrr_ahi_hp4u_aasm15'].transform('first')\n",
    "'''\n",
    "merged_data = merged_data.drop(['nsrr_ahi_hp4u_aasm15 ', 'nsrr_ahi_hp3r_aasm15'], axis=1)\n",
    "\n",
    "'''\n",
    "merged_data = merged_data.fillna(0)\n",
    "\n",
    "merged_data = merged_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process features and labels\n",
    "import numpy as np\n",
    "\n",
    "X = merged_data[['H.R.', 'SaO2', 'ABDO RES', 'THOR RES', 'AIRFLOW']].values\n",
    "Y = merged_data['nsrr_ahi_hp3r_aasm15'].values\n",
    "\n",
    "def categorize_ahi(ahi):\n",
    "  if ahi < 5:\n",
    "    return 0 # no OSA\n",
    "  elif ahi < 15:\n",
    "    return 1 # mild\n",
    "  elif ahi < 30:\n",
    "    return 2 # moderate\n",
    "  else:\n",
    "    return 3 # severe\n",
    "\n",
    "processed_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(categorize_ahi)(ahi) for ahi in tqdm(Y)\n",
    ")\n",
    "\n",
    "Y = np.array(processed_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../transformer_data\"\n",
    "os.makedirs(directory, exist_ok = True)\n",
    "x_dir = os.path.join(directory,\"x.npy\")\n",
    "y_dir = os.path.join(directory,\"y.npy\")\n",
    "np.save(x_dir, X)\n",
    "np.save(y_dir, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data for two-tower Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "visit_path = '../visit_1'\n",
    "visit_files = [f for f in os.listdir(visit_path) if f.endswith('.csv')]\n",
    "\n",
    "aout_path = '../6000_filter'\n",
    "aout_files = [f for f in os.listdir(aout_path) if f.endswith('.csv')]\n",
    "\n",
    "# Read xlxs\n",
    "xlxs_path = '../shhs1_ahi_pruebas.xlsx'\n",
    "xlxs_data = pd.read_excel(xlxs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit\n",
    "def process_visit_csv(csv_file, visit_path):\n",
    "    match = re.search(r'(\\d+)_extraction_(\\d+)', csv_file)\n",
    "    if match:\n",
    "        current_id = match.group(1)\n",
    "        extraction_num = int(match.group(2))\n",
    "        current_data = pd.read_csv(os.path.join(visit_path, csv_file))\n",
    "        current_data['ID'] = current_id\n",
    "        current_data['Extraction'] = extraction_num\n",
    "        return current_data\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 30\n",
    "visit_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(process_visit_csv)(csv_file, visit_path) for csv_file in tqdm(visit_files)\n",
    ")\n",
    "visit_data = pd.concat([df for df in visit_data if not df.empty], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aout\n",
    "def process_aout_csv(csv_file, aout_path):\n",
    "    match = re.search(r'(\\d+)_extraction_(\\d+)', csv_file)\n",
    "    if match:\n",
    "        current_id = match.group(1)\n",
    "        extraction_num = int(match.group(2))\n",
    "        file_path = os.path.join(aout_path, csv_file)\n",
    "        \n",
    "        try:\n",
    "            current_data = pd.read_csv(file_path, header=None)         \n",
    "\n",
    "            current_data = current_data.to_numpy()\n",
    "            id_column = np.full((current_data.shape[0], 1), current_id)\n",
    "            extraction_column = np.full((current_data.shape[0], 1), extraction_num)\n",
    "            combined_data = np.hstack((current_data, extraction_column, id_column))\n",
    "            \n",
    "            return combined_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "            return np.array([])  \n",
    "    else:\n",
    "        return np.array([]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = Parallel(n_jobs=num_processes)(\n",
    "    delayed(process_aout_csv)(csv_file, aout_path)\n",
    "    for csv_file in aout_files\n",
    ")\n",
    "\n",
    "processed_data = [data for data in processed_data if data.size > 0]\n",
    "\n",
    "if processed_data:\n",
    "    all_data = np.vstack(processed_data)\n",
    "    columns = [f'Feature_{i}' for i in range(all_data.shape[1] - 2)] + ['Extraction', 'ID']\n",
    "    aout_data = pd.DataFrame(all_data, columns=columns)\n",
    "    if 'Feature_1872' in aout_data.columns:\n",
    "        aout_data.drop(columns=['Feature_1872'], inplace=True)\n",
    "    print(aout_data.head())\n",
    "else:\n",
    "    aout_data = pd.DataFrame()\n",
    "    print(\"No data processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ids = set(aout_data['ID'].astype(int)).intersection(set(visit_data['ID'].astype(int)))\n",
    "visit_data = visit_data[visit_data['ID'].astype(int).isin(common_ids)]\n",
    "aout_data = aout_data[aout_data['ID'].astype(int).isin(common_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_data = visit_data.fillna(0)\n",
    "aout_data = aout_data.fillna(0)\n",
    "print(visit_data.shape)\n",
    "print(aout_data.shape)\n",
    "print(aout_data['ID'])\n",
    "\n",
    "directory = \"../transformer_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    From original 12 channels to 5 channels. If you directly calculate the Aout matrix (LRIA analysis) \n",
    "    using 5 channels, then ignore this part.\n",
    "\n",
    "\"\"\"\n",
    "# Aout\n",
    "aout_data = aout_data.sort_values(by=['ID', 'Extraction'])\n",
    "print(aout_data['ID'])\n",
    "print(aout_data['Extraction'])\n",
    "\n",
    "# Delete column 'ID' and 'Extraction'\n",
    "aout_data = aout_data.drop(columns=['ID', 'Extraction'])\n",
    "\n",
    "# Delete first 1728 columns\n",
    "aout_data = aout_data.iloc[:, 1728:]\n",
    "\n",
    "selected_columns = [\n",
    "    0, 1, 8, 9, 10, 12, 13, 20, 21, 22, \n",
    "    96, 97, 104, 105, 106, 108, 109, 116, 117, \n",
    "    118, 120, 121, 128, 129, 130\n",
    "]\n",
    "aout_data = aout_data.iloc[:, selected_columns]\n",
    "\n",
    "aout_data_array = aout_data.to_numpy(dtype=float)\n",
    "\n",
    "swap_pairs = [\n",
    "    (0, 1), (2, 3), (5, 6), (7, 8),\n",
    "    (10, 11), (12, 13), (15, 16),\n",
    "    (17, 18), (20, 21), (22, 23)\n",
    "]\n",
    "\n",
    "for i, j in swap_pairs:\n",
    "    aout_data_array[:, [i, j]] = aout_data_array[:, [j, i]]\n",
    "\n",
    "# Visit array\n",
    "aout_data_3d = np.zeros((5556, 5, 5))\n",
    "\n",
    "for i in range(5556):\n",
    "    for j in range(5):\n",
    "        aout_data_3d[i, j, :] = aout_data_array[i, j*5:(j+1)*5]\n",
    "\n",
    "np.save(os.path.join(directory, \"aout_data_3d.npy\"), aout_data_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "unique_ids = sorted(common_ids)\n",
    "num_ids = len(unique_ids)\n",
    "num_extractions = 3 \n",
    "\n",
    "xlxs_data['ID'] = xlxs_data['ID'].astype(int)\n",
    "labels = xlxs_data[xlxs_data['ID'].isin(unique_ids)]\n",
    "labels = labels.set_index('ID').loc[unique_ids, 'nsrr_ahi_hp3r_aasm15'].values\n",
    "\n",
    "def categorize_ahi(ahi):\n",
    "    if ahi < 5:\n",
    "        return 0\n",
    "    elif ahi < 15:\n",
    "        return 1\n",
    "    elif ahi < 30:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "expanded_labels = np.array([categorize_ahi(ahi) for ahi in labels for _ in range(num_extractions)])\n",
    "np.save(os.path.join(directory, \"labels.npy\"), expanded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit\n",
    "channels = ['H.R.', 'SaO2', 'ABDO RES', 'THOR RES', 'AIRFLOW']\n",
    "visit_data = visit_data[visit_data['Extraction'].isin([1, 2, 3])]\n",
    "\n",
    "\n",
    "unique_ids = visit_data['ID'].unique()\n",
    "num_ids = len(unique_ids)\n",
    "num_extractions = 3  \n",
    "num_samples = 36000\n",
    "num_channels = len(channels)\n",
    "\n",
    "\n",
    "visit_data_array = np.zeros((num_ids * num_extractions, num_samples, num_channels))\n",
    "\n",
    "\n",
    "def process_id_data(current_id):\n",
    "    id_data = visit_data[visit_data['ID'] == current_id]\n",
    "    id_data = id_data.sort_values(by=['Extraction', 'ID'])\n",
    "    result = []\n",
    "    for extraction_num in range(1, num_extractions + 1):\n",
    "        extraction_data = id_data[id_data['Extraction'] == extraction_num]\n",
    "        temp_array = np.zeros((num_samples, num_channels))\n",
    "        for j, channel in enumerate(channels):\n",
    "            temp_array[:len(extraction_data), j] = extraction_data[channel].values\n",
    "        result.append(temp_array)\n",
    "    return result\n",
    "\n",
    "processed_results = Parallel(n_jobs=num_processes)(\n",
    "    delayed(process_id_data)(current_id) for current_id in unique_ids\n",
    ")\n",
    "\n",
    "for i, id_result in enumerate(processed_results):\n",
    "    for extraction_num, temp_array in enumerate(id_result):\n",
    "        index = i * num_extractions + extraction_num\n",
    "        visit_data_array[index] = temp_array\n",
    "\n",
    "print(visit_data_array.shape)\n",
    "np.save(os.path.join(directory, \"visit_data.npy\"), visit_data_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3 (main, Apr 19 2023, 23:54:32) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2317310fb273c710267c74ef29c9b393507793e397ff0256299cc1e6ceae5a2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
